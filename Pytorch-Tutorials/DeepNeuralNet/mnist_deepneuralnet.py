# -*- coding: utf-8 -*-
"""MNIST-DeepNeuralNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i5fMFWDjWWI2GttT_ldNY2TCdKhp3dx0
"""

import torch
import numpy as np
import torch.nn as nn
import torchvision
from torchvision.datasets import MNIST

# Download training dataset
dataset = MNIST(root = 'data/', download = True)

import torchvision.transforms as transforms

train_dataset = MNIST(root = 'data/', train = True, transform = transforms.ToTensor())
test_dataset = MNIST(root = 'data/', train = False, transform = transforms.ToTensor())

n = 60000
val_pct = 0.2
def split_indices(n, val_pct):
  # Determine the size of validation set
  n_val = int(n*val_pct)
  # Create the random permutation of 0 to n-1
  idxs = np.random.permutation(n)
  # Pick first n_val indices for validation set
  return idxs[n_val:], idxs[:n_val]

train_indices, val_indices = split_indices(len(train_dataset), val_pct=0.2)

print(len(train_indices), len(val_indices))
print('Sample val indices: ',val_indices[:20])

from torch.utils.data.sampler import SubsetRandomSampler
from torch.utils.data.dataloader import DataLoader

batch_size = 100

# Training sampler and data loader
train_sampler = SubsetRandomSampler(train_indices)
train_loader = DataLoader(train_dataset, batch_size, sampler = train_sampler)

val_sampler = SubsetRandomSampler(val_indices)
val_loader = DataLoader(train_dataset, batch_size, sampler = val_sampler)

for images, labels in train_loader:
  print(labels)
  print(images.shape)
  break

input_size = 28*28
hidden_size = 64
num_classes = 10

class MnistDeepModel(nn.Module):
  def __init__(self,input_size,hidden_size, output_size):
    super().__init__()
    self.linear1 = nn.Linear(input_size,hidden_size)
    self.linear2 = nn.Linear(hidden_size, output_size)
  def forward(self,xb):
    xb = xb.reshape(-1,input_size)
    layer_1 = self.linear1(xb)
    layer1_act = F.relu(layer_1)
    layer_2 = self.linear2(layer1_act)
    return layer_2

model = MnistDeepModel(input_size, hidden_size, num_classes)

import torch.nn.functional as F

loss_fn = F.cross_entropy

learning_rate = 0.001
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

def loss_batch(model, loss_func, xb, yb, opt = None, metric = None):
  # Calculate loss
  preds = model(xb)
  loss = loss_func(preds, yb)
  if opt is not None:
    # Compute gradients
    loss.backward()
    # Updating the gradients
    opt.step()
    # Reset the gradients
    opt.zero_grad()
  metric_result = None
  if metric is not None:
    # Compute metric
    metric_result = metric(preds, yb)
  return loss.item(), len(xb), metric_result

def evaluate(model, loss_func, valid_dl, metric = None):
  with torch.no_grad():
    # Pass each batch through the model
    results = [loss_batch(model, loss_func,xb, yb, metric = metric)
    for xb,yb in valid_dl]
    # Seperate losses, counts and metrics
    losses, nums, metrics = zip(*results)
    # Total size of the dataset
    total = np.sum(nums)
    # Avg loss across batches
    avg_loss = np.sum(np.multiply(losses, nums))/ total
    avg_metric  = None
    if metric is not None:
      # Avg of metric across batches
      avg_metric = np.sum(np.multiply(metrics, nums)) / total
  return avg_loss, total, avg_metric

def accuracy(outputs, labels):
  _, preds = torch.max(outputs, dim=1)
  return torch.sum(preds == labels).item() / len(preds)

val_loss, total, val_acc = evaluate(model, loss_fn, val_loader, metric = accuracy)
print('Loss: {:.4f}, Accuracy: {:.4f}'.format(val_loss, val_acc))

def fit(epochs, model, loss_fn, opt, train_dl, valid_dl, metric = None):
  for epoch in range(epochs):
    # Training
    for xb,yb in train_dl:
      loss,_,_ = loss_batch(model, loss_fn, xb, yb, opt)

    # Evaluation
    result = evaluate(model, loss_fn, valid_dl,metric)
    val_loss, total, val_metric = result

    # Print progress
    if metric is None:
      print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, val_loss))
    else:
      print('Epoch [{}/{}], Loss: {:.4f}, {}: {:.4f}'.format(epoch+1, epochs, val_loss, metric.__name__,val_metric))

fit(80,model, F.cross_entropy, optimizer, train_loader, val_loader, accuracy)

def predict_image(img, model):
  xb = img.unsqueeze(0)
  yb = model(xb)
  _, preds = torch.max(yb, dim=1)
  return preds[0].item()

