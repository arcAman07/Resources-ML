{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST-DeepNeuralNet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2qYj0woUc-4z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision.datasets import MNIST"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download training dataset\n",
        "dataset = MNIST(root = 'data/', download = True)"
      ],
      "metadata": {
        "id": "dh8sdbhSdiWe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "LCyf4l9-diZl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MNIST(root = 'data/', train = True, transform = transforms.ToTensor())\n",
        "test_dataset = MNIST(root = 'data/', train = False, transform = transforms.ToTensor())"
      ],
      "metadata": {
        "id": "2gT5aRbldicg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 60000\n",
        "val_pct = 0.2\n",
        "def split_indices(n, val_pct):\n",
        "  # Determine the size of validation set\n",
        "  n_val = int(n*val_pct)\n",
        "  # Create the random permutation of 0 to n-1\n",
        "  idxs = np.random.permutation(n)\n",
        "  # Pick first n_val indices for validation set\n",
        "  return idxs[n_val:], idxs[:n_val]"
      ],
      "metadata": {
        "id": "HTQziYZJdifQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_indices, val_indices = split_indices(len(train_dataset), val_pct=0.2)"
      ],
      "metadata": {
        "id": "eMSD4r-PdiiJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_indices), len(val_indices))\n",
        "print('Sample val indices: ',val_indices[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sClDklT5dikt",
        "outputId": "d4221093-da4b-4712-acfb-670c4687f394"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48000 12000\n",
            "Sample val indices:  [56126 32417  1139 17614 47999 51769 55729  9498  9298 19659  8908 44359\n",
            " 10678 39190 20490 34771 30571 36613 38717 10158]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data.dataloader import DataLoader"
      ],
      "metadata": {
        "id": "VbL-dDZsdinb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "\n",
        "# Training sampler and data loader\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "train_loader = DataLoader(train_dataset, batch_size, sampler = train_sampler)\n",
        "\n",
        "val_sampler = SubsetRandomSampler(val_indices)\n",
        "val_loader = DataLoader(train_dataset, batch_size, sampler = val_sampler)"
      ],
      "metadata": {
        "id": "OCiBrfTrdiqE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in train_loader:\n",
        "  print(labels)\n",
        "  print(images.shape)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbGc2nk4dis1",
        "outputId": "60272c06-7ea4-4e19-95a8-36865d4d09ea"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3, 2, 8, 6, 9, 9, 1, 0, 8, 8, 0, 9, 0, 1, 7, 8, 2, 7, 0, 2, 2, 3, 5, 5,\n",
            "        3, 9, 5, 2, 5, 4, 0, 9, 9, 7, 9, 7, 2, 1, 8, 8, 1, 3, 6, 6, 5, 4, 8, 1,\n",
            "        4, 0, 2, 3, 0, 7, 1, 9, 9, 5, 8, 1, 1, 1, 8, 9, 2, 0, 4, 2, 6, 4, 9, 2,\n",
            "        5, 5, 5, 6, 4, 6, 4, 0, 3, 5, 8, 2, 7, 7, 8, 3, 5, 6, 3, 6, 8, 1, 5, 4,\n",
            "        1, 4, 3, 1])\n",
            "torch.Size([100, 1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 28*28\n",
        "hidden_size = 64\n",
        "num_classes = 10"
      ],
      "metadata": {
        "id": "t2NvOMxmI7Dr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MnistDeepModel(nn.Module):\n",
        "  def __init__(self,input_size,hidden_size, output_size):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(input_size,hidden_size)\n",
        "    self.linear2 = nn.Linear(hidden_size, output_size)\n",
        "  def forward(self,xb):\n",
        "    xb = xb.reshape(-1,input_size)\n",
        "    layer_1 = self.linear1(xb)\n",
        "    layer1_act = F.relu(layer_1)\n",
        "    layer_2 = self.linear2(layer1_act)\n",
        "    return layer_2"
      ],
      "metadata": {
        "id": "HUzs_COxdiyo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MnistDeepModel(input_size, hidden_size, num_classes)"
      ],
      "metadata": {
        "id": "wZnyiS0vKgEm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "wvdH0_xvK7Eu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = F.cross_entropy"
      ],
      "metadata": {
        "id": "SYYocl1Pdi7M"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "xjDaTKpxdi-G"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_batch(model, loss_func, xb, yb, opt = None, metric = None):\n",
        "  # Calculate loss\n",
        "  preds = model(xb)\n",
        "  loss = loss_func(preds, yb)\n",
        "  if opt is not None:\n",
        "    # Compute gradients\n",
        "    loss.backward()\n",
        "    # Updating the gradients\n",
        "    opt.step()\n",
        "    # Reset the gradients\n",
        "    opt.zero_grad()\n",
        "  metric_result = None\n",
        "  if metric is not None:\n",
        "    # Compute metric\n",
        "    metric_result = metric(preds, yb)\n",
        "  return loss.item(), len(xb), metric_result"
      ],
      "metadata": {
        "id": "61zzwokudjAx"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loss_func, valid_dl, metric = None):\n",
        "  with torch.no_grad():\n",
        "    # Pass each batch through the model\n",
        "    results = [loss_batch(model, loss_func,xb, yb, metric = metric)\n",
        "    for xb,yb in valid_dl]\n",
        "    # Seperate losses, counts and metrics\n",
        "    losses, nums, metrics = zip(*results)\n",
        "    # Total size of the dataset\n",
        "    total = np.sum(nums)\n",
        "    # Avg loss across batches\n",
        "    avg_loss = np.sum(np.multiply(losses, nums))/ total\n",
        "    avg_metric  = None\n",
        "    if metric is not None:\n",
        "      # Avg of metric across batches\n",
        "      avg_metric = np.sum(np.multiply(metrics, nums)) / total\n",
        "  return avg_loss, total, avg_metric"
      ],
      "metadata": {
        "id": "W4tA9zOGdjDq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(outputs, labels):\n",
        "  _, preds = torch.max(outputs, dim=1)\n",
        "  return torch.sum(preds == labels).item() / len(preds)"
      ],
      "metadata": {
        "id": "G7ez9vJ2djGX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss, total, val_acc = evaluate(model, loss_fn, val_loader, metric = accuracy)\n",
        "print('Loss: {:.4f}, Accuracy: {:.4f}'.format(val_loss, val_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HplfvKlpdjMH",
        "outputId": "c984144d-127f-4c32-e9c4-1db5523c0077"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 2.3226, Accuracy: 0.0899\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(epochs, model, loss_fn, opt, train_dl, valid_dl, metric = None):\n",
        "  for epoch in range(epochs):\n",
        "    # Training\n",
        "    for xb,yb in train_dl:\n",
        "      loss,_,_ = loss_batch(model, loss_fn, xb, yb, opt)\n",
        "\n",
        "    # Evaluation\n",
        "    result = evaluate(model, loss_fn, valid_dl,metric)\n",
        "    val_loss, total, val_metric = result\n",
        "\n",
        "    # Print progress\n",
        "    if metric is None:\n",
        "      print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, val_loss))\n",
        "    else:\n",
        "      print('Epoch [{}/{}], Loss: {:.4f}, {}: {:.4f}'.format(epoch+1, epochs, val_loss, metric.__name__,val_metric))"
      ],
      "metadata": {
        "id": "wo3C6YnydjPP"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fit(80,model, F.cross_entropy, optimizer, train_loader, val_loader, accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OjRiPZndjSl",
        "outputId": "440c771d-a247-489b-9d6b-a02968de02db"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/80], Loss: 2.2303, accuracy: 0.2200\n",
            "Epoch [2/80], Loss: 2.1238, accuracy: 0.4525\n",
            "Epoch [3/80], Loss: 1.9984, accuracy: 0.6130\n",
            "Epoch [4/80], Loss: 1.8549, accuracy: 0.6889\n",
            "Epoch [5/80], Loss: 1.6979, accuracy: 0.7222\n",
            "Epoch [6/80], Loss: 1.5382, accuracy: 0.7446\n",
            "Epoch [7/80], Loss: 1.3867, accuracy: 0.7647\n",
            "Epoch [8/80], Loss: 1.2512, accuracy: 0.7783\n",
            "Epoch [9/80], Loss: 1.1347, accuracy: 0.7926\n",
            "Epoch [10/80], Loss: 1.0368, accuracy: 0.8030\n",
            "Epoch [11/80], Loss: 0.9553, accuracy: 0.8131\n",
            "Epoch [12/80], Loss: 0.8875, accuracy: 0.8194\n",
            "Epoch [13/80], Loss: 0.8308, accuracy: 0.8253\n",
            "Epoch [14/80], Loss: 0.7831, accuracy: 0.8295\n",
            "Epoch [15/80], Loss: 0.7424, accuracy: 0.8336\n",
            "Epoch [16/80], Loss: 0.7076, accuracy: 0.8386\n",
            "Epoch [17/80], Loss: 0.6774, accuracy: 0.8419\n",
            "Epoch [18/80], Loss: 0.6510, accuracy: 0.8453\n",
            "Epoch [19/80], Loss: 0.6278, accuracy: 0.8492\n",
            "Epoch [20/80], Loss: 0.6072, accuracy: 0.8522\n",
            "Epoch [21/80], Loss: 0.5888, accuracy: 0.8542\n",
            "Epoch [22/80], Loss: 0.5722, accuracy: 0.8578\n",
            "Epoch [23/80], Loss: 0.5573, accuracy: 0.8602\n",
            "Epoch [24/80], Loss: 0.5437, accuracy: 0.8618\n",
            "Epoch [25/80], Loss: 0.5313, accuracy: 0.8638\n",
            "Epoch [26/80], Loss: 0.5200, accuracy: 0.8672\n",
            "Epoch [27/80], Loss: 0.5097, accuracy: 0.8687\n",
            "Epoch [28/80], Loss: 0.5001, accuracy: 0.8705\n",
            "Epoch [29/80], Loss: 0.4912, accuracy: 0.8719\n",
            "Epoch [30/80], Loss: 0.4831, accuracy: 0.8738\n",
            "Epoch [31/80], Loss: 0.4753, accuracy: 0.8754\n",
            "Epoch [32/80], Loss: 0.4683, accuracy: 0.8771\n",
            "Epoch [33/80], Loss: 0.4616, accuracy: 0.8788\n",
            "Epoch [34/80], Loss: 0.4554, accuracy: 0.8802\n",
            "Epoch [35/80], Loss: 0.4495, accuracy: 0.8806\n",
            "Epoch [36/80], Loss: 0.4441, accuracy: 0.8820\n",
            "Epoch [37/80], Loss: 0.4389, accuracy: 0.8834\n",
            "Epoch [38/80], Loss: 0.4340, accuracy: 0.8848\n",
            "Epoch [39/80], Loss: 0.4293, accuracy: 0.8850\n",
            "Epoch [40/80], Loss: 0.4250, accuracy: 0.8857\n",
            "Epoch [41/80], Loss: 0.4208, accuracy: 0.8868\n",
            "Epoch [42/80], Loss: 0.4169, accuracy: 0.8872\n",
            "Epoch [43/80], Loss: 0.4132, accuracy: 0.8878\n",
            "Epoch [44/80], Loss: 0.4096, accuracy: 0.8884\n",
            "Epoch [45/80], Loss: 0.4061, accuracy: 0.8890\n",
            "Epoch [46/80], Loss: 0.4028, accuracy: 0.8894\n",
            "Epoch [47/80], Loss: 0.3997, accuracy: 0.8901\n",
            "Epoch [48/80], Loss: 0.3967, accuracy: 0.8903\n",
            "Epoch [49/80], Loss: 0.3939, accuracy: 0.8906\n",
            "Epoch [50/80], Loss: 0.3912, accuracy: 0.8912\n",
            "Epoch [51/80], Loss: 0.3885, accuracy: 0.8918\n",
            "Epoch [52/80], Loss: 0.3860, accuracy: 0.8922\n",
            "Epoch [53/80], Loss: 0.3836, accuracy: 0.8932\n",
            "Epoch [54/80], Loss: 0.3811, accuracy: 0.8936\n",
            "Epoch [55/80], Loss: 0.3789, accuracy: 0.8943\n",
            "Epoch [56/80], Loss: 0.3766, accuracy: 0.8948\n",
            "Epoch [57/80], Loss: 0.3746, accuracy: 0.8960\n",
            "Epoch [58/80], Loss: 0.3725, accuracy: 0.8966\n",
            "Epoch [59/80], Loss: 0.3706, accuracy: 0.8969\n",
            "Epoch [60/80], Loss: 0.3686, accuracy: 0.8975\n",
            "Epoch [61/80], Loss: 0.3667, accuracy: 0.8978\n",
            "Epoch [62/80], Loss: 0.3649, accuracy: 0.8982\n",
            "Epoch [63/80], Loss: 0.3632, accuracy: 0.8982\n",
            "Epoch [64/80], Loss: 0.3614, accuracy: 0.8987\n",
            "Epoch [65/80], Loss: 0.3597, accuracy: 0.8998\n",
            "Epoch [66/80], Loss: 0.3582, accuracy: 0.8996\n",
            "Epoch [67/80], Loss: 0.3566, accuracy: 0.9006\n",
            "Epoch [68/80], Loss: 0.3550, accuracy: 0.9008\n",
            "Epoch [69/80], Loss: 0.3535, accuracy: 0.9013\n",
            "Epoch [70/80], Loss: 0.3520, accuracy: 0.9017\n",
            "Epoch [71/80], Loss: 0.3507, accuracy: 0.9016\n",
            "Epoch [72/80], Loss: 0.3493, accuracy: 0.9023\n",
            "Epoch [73/80], Loss: 0.3479, accuracy: 0.9025\n",
            "Epoch [74/80], Loss: 0.3465, accuracy: 0.9032\n",
            "Epoch [75/80], Loss: 0.3453, accuracy: 0.9031\n",
            "Epoch [76/80], Loss: 0.3440, accuracy: 0.9037\n",
            "Epoch [77/80], Loss: 0.3427, accuracy: 0.9042\n",
            "Epoch [78/80], Loss: 0.3415, accuracy: 0.9040\n",
            "Epoch [79/80], Loss: 0.3403, accuracy: 0.9042\n",
            "Epoch [80/80], Loss: 0.3392, accuracy: 0.9046\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_image(img, model):\n",
        "  xb = img.unsqueeze(0)\n",
        "  yb = model(xb)\n",
        "  _, preds = torch.max(yb, dim=1)\n",
        "  return preds[0].item()"
      ],
      "metadata": {
        "id": "f2jsJQe2LS-O"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uucvLnIBM7LO"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}