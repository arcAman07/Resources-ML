# -*- coding: utf-8 -*-
"""LinearRegression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xtnofb-JqU172H94_fBJ3YefufTpp4oX
"""

import numpy as np
import torch

# Training data
# [Temp, Rainfall, Humidity]
inputs = np.array([[73,67,43],[91,88,64],[87,134,58],[102,43,37],[69,96,70]],dtype='float32')
# [Apples, Oranges]
targets = np.array([[56,70],[81,101],[119,133],[22,37],[103,119]], dtype='float32')

# Weights and biases
w = torch.randn(2,3,requires_grad=True)
b = torch.randn(2,requires_grad=True)
print(w)
print(b)

def model(x):
  return torch.matmul(x,torch.transpose(w,0,1)) + b

preds = model(torch.from_numpy(inputs))
print(preds)

print(targets)

def mse(t1, t2):
  diff = t1-t2
  return torch.sum(diff*diff)/diff.numel()

loss = mse(preds,torch.from_numpy(targets))
print(loss)

# Compute gradients
loss.backward()

# Gradients for weights
print(w)
print(w.grad)

# Gradients for bias values
print(b)
print(b.grad)

# ADJUST WEIGHTS AND BIASES USING GRADIENT DESCENT
with torch.no_grad():
  w -= w.grad * 1e-5
  b -= b.grad * 1e-5
  w.grad.zero_()
  b.grad.zero_()

print(w)
print(b)

# Train for multiple epochs
for i in range(0,3000):
  preds = model(torch.from_numpy(inputs))
  loss =  mse(preds,torch.from_numpy(targets))
  loss.backward()
  with torch.no_grad():
    w -= w.grad * 1e-5
    b -= b.grad * 1e-5
    w.grad.zero_()
    b.grad.zero_()

new_preds = model(torch.from_numpy(inputs))

new_loss = mse(new_preds,torch.from_numpy(targets))

print(new_loss)

print(new_preds)

print(w)
print(b)